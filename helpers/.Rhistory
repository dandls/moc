ffnet %>%
compile(optimizer = optimizer_adam(lr),
loss = "binary_crossentropy",
metrics = "accuracy")
}
#--- Train Models ----
# Loop over learners
resampling = rsmp("cv", folds = 3)
measure = msr("classif.ce")
tuner = tnr("grid_search", resolution = n_evals)
terminator = term("evals", n_evals = n_evals)
learners = list(
GraphLearner$new(
{
mod = po("scale") %>>%
po("encode") %>>%
po(lrn("classif.gbm", predict_type = "prob", nthread = 1L))
mod$keep_results = TRUE
tune_ps = paradox::ParamSet$new(list(
paradox::ParamDbl$new("classif.gbm.n.trees", lower = 1,
upper = log(1000))))
tune_ps$trafo = function(x, param_set) {
x$x$classif.gbm.n.trees = round(exp(x$x$classif.gbm.n.trees))
return(x)
}
mod = GraphLearner$new(mod)
AutoTuner$new(mod, resampling, measure, tune_ps, terminator, tuner)
}
),
GraphLearner$new(
{
mod = po("scale") %>>%
po("encode") %>>%
po(lrn("classif.svm", predict_type = "prob", type = "C-classification"))
mod$keep_results = TRUE
tune_ps = paradox::ParamSet$new(list(
paradox::ParamDbl$new("classif.svm.cost", lower = 0.01, upper = 1)))
mod = GraphLearner$new(mod)
AutoTuner$new(mod, resampling, measure, tune_ps, terminator, tuner)
}
),
# GraphLearner$new(
#   {
#     mod = po("scale") %>>%
#       po("encode") %>>%
#       po(lrn("classif.kerasff", predict_type = "prob", layer_units = integer(),
#         epochs = 100L, activation = "sigmoid", use_batchnorm = FALSE,
#         optimizer = "adam", loss = "binary_crossentropy", metrics = "accuracy",
#         callbacks = list(cb_es(5))))
#     mod$keep_results = TRUE
#     mod
#   }
# ),
# GraphLearner$new(
#   {
#     mod = po("scale") %>>%
#       po("encode") %>>%
#       po(lrn("classif.keras", model = NULL, epochs = 10L, predict_type = "prob",
#         callbacks = list(cb_es(5))))
#     mod$keep_results = TRUE
#     tune_ps = paradox::ParamSet$new(list(
#       paradox:: ParamFct$new("classif.keras.model", levels = c("arch1", "arch2", "arch3", "arch4"),
#         tags = "train"),
#       paradox::ParamDbl$new("classif.keras.lr", lower = 10^-5, upper = 10^-1, tags = "train")
#     ))
#
#     tune_ps$trafo = function(x, param_set) {
#       x$classif.keras.model = get_keras_model(x$classif.keras.model, x$classif.keras.lr)
#       x$classif.keras.lr = NULL
#       return(x)
#     }
#     mod = GraphLearner$new(mod)
#     AutoTuner$new(mod, resampling, measure, tune_ps, terminator, tuner)
#   }
# ),
GraphLearner$new(
{
mod = po("scale") %>>%
po("encode") %>>%
po(lrn("classif.ranger", predict_type = "prob"))
mod$keep_results = TRUE
tune_ps =  paradox::ParamSet$new(list(
paradox::ParamDbl$new("classif.ranger.num.trees", lower = 1, upper = log(1000))))
tune_ps$trafo = function(x, param_set) {
x$classif.ranger.num.trees = round(exp(x$classif.ranger.num.trees))
return(x)
}
mod = GraphLearner$new(mod)
AutoTuner$new(mod, resampling, measure, tune_ps, terminator, tuner)
})
)
packages = c("rstudioapi", "ranger", "mlr3", "mlr3learners", "mlr3pipelines", "mlr3keras", "OpenML",
"mlr3misc", "mlr3tuning", "e1071", "stringr", "gbm",
"paradox", "checkmate", "keras", "rjson", "parallelMap")
sapply(packages, require, character.only = TRUE)
#--- Set path ----
current_dir = getwd()
data_dir = file.path(current_dir, "..", "saved_objects", "data")
dir.create(path = data_dir, showWarnings = FALSE)
#--- Task design ----
task_ids = c( 3718, 3749)
task_list = lapply(task_ids, function(x) {
t = getOMLTask(task.id = x)
df = t$input$data.set$data
target = t$input$target.features
positive = as.character(unique(t$input$data.set$data[, target][1]))
task = mlr3::TaskClassif$new(id = as.character(t$task.id), backend = df,
target = target, positive = positive)
})
names(task_list) = task_ids
#--- Utility functions ----
get_keras_model = function(arch = "arch1", lr = 3*10^-4) {
if (arch == "arch1") {
ffnet= keras_model_sequential()
ffnet%>%
layer_dense(units = 2^4, activation = "relu") %>%
layer_dense(units = 2^4, activation = "relu") %>%
layer_dense(units = 1L, activation = "sigmoid")
} else if (arch == "arch2") {
ffnet= keras_model_sequential()
ffnet%>%
layer_dense(units = 2^5, activation = "relu") %>%
layer_dense(units = 2^5, activation = "relu") %>%
layer_dense(units = 1L, activation = "sigmoid")
} else if (arch == "arch3") {
ffnet= keras_model_sequential()
ffnet%>%
layer_dense(units = 2^6, activation = "relu") %>%
layer_dense(units = 2^6, activation = "relu") %>%
layer_dense(units = 1L, activation = "sigmoid")
} else if (arch == "arch4") {
ffnet= keras_model_sequential()
ffnet%>%
layer_dense(units = 2^7, activation = "relu") %>%
layer_dense(units = 2^7, activation = "relu") %>%
layer_dense(units = 1L, activation = "sigmoid")
}
ffnet %>%
compile(optimizer = optimizer_adam(lr),
loss = "binary_crossentropy",
metrics = "accuracy")
}
resampling = rsmp("cv", folds = 3)
measure = msr("classif.ce")
tuner = tnr("grid_search", resolution = n_evals)
terminator = term("evals", n_evals = n_evals)
learners = list(
GraphLearner$new(
{
mod = po("scale") %>>%
po("encode") %>>%
po(lrn("classif.gbm", predict_type = "prob", nthread = 1L))
mod$keep_results = TRUE
tune_ps = paradox::ParamSet$new(list(
paradox::ParamDbl$new("classif.gbm.n.trees", lower = 1,
upper = log(1000))))
tune_ps$trafo = function(x, param_set) {
x$x$classif.gbm.n.trees = round(exp(x$x$classif.gbm.n.trees))
return(x)
}
mod = GraphLearner$new(mod)
AutoTuner$new(mod, resampling, measure, tune_ps, terminator, tuner)
}
),
GraphLearner$new(
{
mod = po("scale") %>>%
po("encode") %>>%
po(lrn("classif.svm", predict_type = "prob", type = "C-classification"))
mod$keep_results = TRUE
tune_ps = paradox::ParamSet$new(list(
paradox::ParamDbl$new("classif.svm.cost", lower = 0.01, upper = 1)))
mod = GraphLearner$new(mod)
AutoTuner$new(mod, resampling, measure, tune_ps, terminator, tuner)
}
),
# GraphLearner$new(
#   {
#     mod = po("scale") %>>%
#       po("encode") %>>%
#       po(lrn("classif.kerasff", predict_type = "prob", layer_units = integer(),
#         epochs = 100L, activation = "sigmoid", use_batchnorm = FALSE,
#         optimizer = "adam", loss = "binary_crossentropy", metrics = "accuracy",
#         callbacks = list(cb_es(5))))
#     mod$keep_results = TRUE
#     mod
#   }
# ),
# GraphLearner$new(
#   {
#     mod = po("scale") %>>%
#       po("encode") %>>%
#       po(lrn("classif.keras", model = NULL, epochs = 10L, predict_type = "prob",
#         callbacks = list(cb_es(5))))
#     mod$keep_results = TRUE
#     tune_ps = paradox::ParamSet$new(list(
#       paradox:: ParamFct$new("classif.keras.model", levels = c("arch1", "arch2", "arch3", "arch4"),
#         tags = "train"),
#       paradox::ParamDbl$new("classif.keras.lr", lower = 10^-5, upper = 10^-1, tags = "train")
#     ))
#
#     tune_ps$trafo = function(x, param_set) {
#       x$classif.keras.model = get_keras_model(x$classif.keras.model, x$classif.keras.lr)
#       x$classif.keras.lr = NULL
#       return(x)
#     }
#     mod = GraphLearner$new(mod)
#     AutoTuner$new(mod, resampling, measure, tune_ps, terminator, tuner)
#   }
# ),
GraphLearner$new(
{
mod = po("scale") %>>%
po("encode") %>>%
po(lrn("classif.ranger", predict_type = "prob"))
mod$keep_results = TRUE
tune_ps =  paradox::ParamSet$new(list(
paradox::ParamDbl$new("classif.ranger.num.trees", lower = 1, upper = log(1000))))
tune_ps$trafo = function(x, param_set) {
x$classif.ranger.num.trees = round(exp(x$classif.ranger.num.trees))
return(x)
}
mod = GraphLearner$new(mod)
AutoTuner$new(mod, resampling, measure, tune_ps, terminator, tuner)
})
)
remotes::install_github("mlr-org/mlr3learners")
n_evals = 2L
#--- Install packages if not already installed ----
packages = c("rstudioapi", "ranger", "mlr3", "mlr3learners", "mlr3pipelines", "mlr3keras", "OpenML",
"mlr3misc", "mlr3tuning", "e1071", "stringr", "gbm",
"paradox", "checkmate", "keras", "rjson", "parallelMap")
sapply(packages, require, character.only = TRUE)
#--- Set path ----
current_dir = getwd()
data_dir = file.path(current_dir, "..", "saved_objects", "data")
dir.create(path = data_dir, showWarnings = FALSE)
#--- Task design ----
task_ids = c( 3718, 3749)
task_list = lapply(task_ids, function(x) {
t = getOMLTask(task.id = x)
df = t$input$data.set$data
target = t$input$target.features
positive = as.character(unique(t$input$data.set$data[, target][1]))
task = mlr3::TaskClassif$new(id = as.character(t$task.id), backend = df,
target = target, positive = positive)
})
names(task_list) = task_ids
#--- Utility functions ----
get_keras_model = function(arch = "arch1", lr = 3*10^-4) {
if (arch == "arch1") {
ffnet= keras_model_sequential()
ffnet%>%
layer_dense(units = 2^4, activation = "relu") %>%
layer_dense(units = 2^4, activation = "relu") %>%
layer_dense(units = 1L, activation = "sigmoid")
} else if (arch == "arch2") {
ffnet= keras_model_sequential()
ffnet%>%
layer_dense(units = 2^5, activation = "relu") %>%
layer_dense(units = 2^5, activation = "relu") %>%
layer_dense(units = 1L, activation = "sigmoid")
} else if (arch == "arch3") {
ffnet= keras_model_sequential()
ffnet%>%
layer_dense(units = 2^6, activation = "relu") %>%
layer_dense(units = 2^6, activation = "relu") %>%
layer_dense(units = 1L, activation = "sigmoid")
} else if (arch == "arch4") {
ffnet= keras_model_sequential()
ffnet%>%
layer_dense(units = 2^7, activation = "relu") %>%
layer_dense(units = 2^7, activation = "relu") %>%
layer_dense(units = 1L, activation = "sigmoid")
}
ffnet %>%
compile(optimizer = optimizer_adam(lr),
loss = "binary_crossentropy",
metrics = "accuracy")
}
resampling = rsmp("cv", folds = 3)
measure = msr("classif.ce")
tuner = tnr("grid_search", resolution = n_evals)
terminator = term("evals", n_evals = n_evals)
learners = list(
GraphLearner$new(
{
mod = po("scale") %>>%
po("encode") %>>%
po(lrn("classif.gbm", predict_type = "prob", nthread = 1L))
mod$keep_results = TRUE
tune_ps = paradox::ParamSet$new(list(
paradox::ParamDbl$new("classif.gbm.n.trees", lower = 1,
upper = log(1000))))
tune_ps$trafo = function(x, param_set) {
x$x$classif.gbm.n.trees = round(exp(x$x$classif.gbm.n.trees))
return(x)
}
mod = GraphLearner$new(mod)
AutoTuner$new(mod, resampling, measure, tune_ps, terminator, tuner)
}
),
GraphLearner$new(
{
mod = po("scale") %>>%
po("encode") %>>%
po(lrn("classif.svm", predict_type = "prob", type = "C-classification"))
mod$keep_results = TRUE
tune_ps = paradox::ParamSet$new(list(
paradox::ParamDbl$new("classif.svm.cost", lower = 0.01, upper = 1)))
mod = GraphLearner$new(mod)
AutoTuner$new(mod, resampling, measure, tune_ps, terminator, tuner)
}
),
# GraphLearner$new(
#   {
#     mod = po("scale") %>>%
#       po("encode") %>>%
#       po(lrn("classif.kerasff", predict_type = "prob", layer_units = integer(),
#         epochs = 100L, activation = "sigmoid", use_batchnorm = FALSE,
#         optimizer = "adam", loss = "binary_crossentropy", metrics = "accuracy",
#         callbacks = list(cb_es(5))))
#     mod$keep_results = TRUE
#     mod
#   }
# ),
# GraphLearner$new(
#   {
#     mod = po("scale") %>>%
#       po("encode") %>>%
#       po(lrn("classif.keras", model = NULL, epochs = 10L, predict_type = "prob",
#         callbacks = list(cb_es(5))))
#     mod$keep_results = TRUE
#     tune_ps = paradox::ParamSet$new(list(
#       paradox:: ParamFct$new("classif.keras.model", levels = c("arch1", "arch2", "arch3", "arch4"),
#         tags = "train"),
#       paradox::ParamDbl$new("classif.keras.lr", lower = 10^-5, upper = 10^-1, tags = "train")
#     ))
#
#     tune_ps$trafo = function(x, param_set) {
#       x$classif.keras.model = get_keras_model(x$classif.keras.model, x$classif.keras.lr)
#       x$classif.keras.lr = NULL
#       return(x)
#     }
#     mod = GraphLearner$new(mod)
#     AutoTuner$new(mod, resampling, measure, tune_ps, terminator, tuner)
#   }
# ),
GraphLearner$new(
{
mod = po("scale") %>>%
po("encode") %>>%
po(lrn("classif.ranger", predict_type = "prob"))
mod$keep_results = TRUE
tune_ps =  paradox::ParamSet$new(list(
paradox::ParamDbl$new("classif.ranger.num.trees", lower = 1, upper = log(1000))))
tune_ps$trafo = function(x, param_set) {
x$classif.ranger.num.trees = round(exp(x$classif.ranger.num.trees))
return(x)
}
mod = GraphLearner$new(mod)
AutoTuner$new(mod, resampling, measure, tune_ps, terminator, tuner)
})
)
remotes::install_github("mlr-org/mlr3learners/mlr3learners.gbm")
remotes::install_github("mlr-org/mlr3learners.gbm")
remotes::install_github("mlr-org/mlr3learners")
remotes::install_github("mlr3learners")
remotes::install_github("mlr-org/mlr3learners")
0
remotes::install_github("mlr-org/mlr3learners.gbm")
####################################
## Train Models
####################################
n_evals = 2L
#--- Install packages if not already installed ----
packages = c("rstudioapi", "ranger", "mlr3", "mlr3learners", "mlr3pipelines", "mlr3keras", "OpenML",
"mlr3misc", "mlr3tuning", "e1071", "stringr", "future.apply",
"paradox", "checkmate", "keras", "rjson", "parallelMap")
sapply(packages, require, character.only = TRUE)
#--- Set path ----
current_dir = getwd()
data_dir = file.path(current_dir, "..", "saved_objects", "data")
dir.create(path = data_dir, showWarnings = FALSE)
#--- Task design ----
task_ids = c( 3718, 3749)
task_list = lapply(task_ids, function(x) {
t = getOMLTask(task.id = x)
df = t$input$data.set$data
target = t$input$target.features
positive = as.character(unique(t$input$data.set$data[, target][1]))
task = mlr3::TaskClassif$new(id = as.character(t$task.id), backend = df,
target = target, positive = positive)
})
names(task_list) = task_ids
#--- Utility functions ----
get_keras_model = function(arch = "arch1", lr = 3*10^-4) {
if (arch == "arch1") {
ffnet= keras_model_sequential()
ffnet%>%
layer_dense(units = 2^4, activation = "relu") %>%
layer_dense(units = 2^4, activation = "relu") %>%
layer_dense(units = 1L, activation = "sigmoid")
} else if (arch == "arch2") {
ffnet= keras_model_sequential()
ffnet%>%
layer_dense(units = 2^5, activation = "relu") %>%
layer_dense(units = 2^5, activation = "relu") %>%
layer_dense(units = 1L, activation = "sigmoid")
} else if (arch == "arch3") {
ffnet= keras_model_sequential()
ffnet%>%
layer_dense(units = 2^6, activation = "relu") %>%
layer_dense(units = 2^6, activation = "relu") %>%
layer_dense(units = 1L, activation = "sigmoid")
} else if (arch == "arch4") {
ffnet= keras_model_sequential()
ffnet%>%
layer_dense(units = 2^7, activation = "relu") %>%
layer_dense(units = 2^7, activation = "relu") %>%
layer_dense(units = 1L, activation = "sigmoid")
}
ffnet %>%
compile(optimizer = optimizer_adam(lr),
loss = "binary_crossentropy",
metrics = "accuracy")
}
#--- Train Models ----
# Loop over learners
resampling = rsmp("cv", folds = 3)
measure = msr("classif.ce")
tuner = tnr("grid_search", resolution = n_evals)
terminator = term("evals", n_evals = n_evals)
learners = list(
GraphLearner$new(
{
mod = po("scale") %>>%
po("encode") %>>%
po(lrn("classif.xgboost", predict_type = "prob", nthread = 1L))
mod$keep_results = TRUE
tune_ps = paradox::ParamSet$new(list(
paradox::ParamDbl$new("classif.xgboost.nrounds", lower = 1,
upper = log(1000))))
tune_ps$trafo = function(x, param_set) {
x$classif.xgboost.nrounds = round(exp(x$classif.xgboost.nrounds))
return(x)
}
mod = GraphLearner$new(mod)
AutoTuner$new(mod, resampling, measure, tune_ps, terminator, tuner)
}
),
GraphLearner$new(
{
mod = po("scale") %>>%
po("encode") %>>%
po(lrn("classif.svm", predict_type = "prob", type = "C-classification"))
mod$keep_results = TRUE
tune_ps = paradox::ParamSet$new(list(
paradox::ParamDbl$new("classif.svm.cost", lower = 0.01, upper = 1)))
mod = GraphLearner$new(mod)
AutoTuner$new(mod, resampling, measure, tune_ps, terminator, tuner)
}
),
GraphLearner$new(
{
mod = po("scale") %>>%
po("encode") %>>%
po(lrn("classif.kerasff", predict_type = "prob", layer_units = integer(),
epochs = 100L, activation = "sigmoid", use_batchnorm = FALSE,
optimizer = "adam", loss = "binary_crossentropy", metrics = "accuracy",
callbacks = list(cb_es(5))))
mod$keep_results = TRUE
mod
}
),
GraphLearner$new(
{
mod = po("scale") %>>%
po("encode") %>>%
po(lrn("classif.keras", model = NULL, epochs = 10L, predict_type = "prob",
callbacks = list(cb_es(5))))
mod$keep_results = TRUE
tune_ps = paradox::ParamSet$new(list(
paradox:: ParamFct$new("classif.keras.model", levels = c("arch1", "arch2", "arch3", "arch4"),
tags = "train"),
paradox::ParamDbl$new("classif.keras.lr", lower = 10^-5, upper = 10^-1, tags = "train")
))
tune_ps$trafo = function(x, param_set) {
x$classif.keras.model = get_keras_model(x$classif.keras.model, x$classif.keras.lr)
x$classif.keras.lr = NULL
return(x)
}
mod = GraphLearner$new(mod)
AutoTuner$new(mod, resampling, measure, tune_ps, terminator, tuner)
}
),
GraphLearner$new(
{
mod = po("scale") %>>%
po("encode") %>>%
po(lrn("classif.ranger", predict_type = "prob"))
mod$keep_results = TRUE
tune_ps =  paradox::ParamSet$new(list(
paradox::ParamDbl$new("classif.ranger.num.trees", lower = 1, upper = log(1000))))
tune_ps$trafo = function(x, param_set) {
x$classif.ranger.num.trees = round(exp(x$classif.ranger.num.trees))
return(x)
}
mod = GraphLearner$new(mod)
AutoTuner$new(mod, resampling, measure, tune_ps, terminator, tuner)
})
)
