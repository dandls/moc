makeIntegerLearnerParam(id = "epochs", default = 30L, lower = 1L)),
par.vals = list(num.threads = 1L, verbose = FALSE, respect.unordered.factors = "order"),
properties = c("twoclass", "numerics", "factors", "prob",
"ordered"),
name = "Keras Logistic Regression",
short.name = "keraslogreg",
note = "Learner param 'predict.method' maps to 'method' in predict.keraslogreg."
)
}
trans_target = function(target) {
browser()
y = to_categorical(as.integer(target) - 1)
y = y[, 1, drop = FALSE]
return(y)
}
trainLearner.classif.keraslogreg = function(.learner, .task, .subset, .weights = NULL, ...) {
dt = getTaskData(.task)
x = as.matrix(dt[, getTaskFeatureNames(.task)])
target = dt[, getTaskTargetNames(.task)]
y = trans_target(target)
input_shape = ncol(data)
target_labels = getTaskClassLevels(.task)
output_shape = length(target_labels) - 1
model = keras_model_sequential()
model = model %>%
layer_dense(
units = output_shape,
input_shape = input_shape,
activation = "sigmoid")
compile(model, optimizer = "adam", loss = "binary_crossentropy",
metrics = "accuracy")
es = callback_early_stopping(monitor='val_loss', patience=5L)
history = invoke(keras::fit,
object = model,
x = x,
y = y,
epochs = .learner$par.vals$epochs,
batch_size = 128,
validation_split =1/3,
callbacks = list(es))
return(list(model = model, history = history, target_labels = target_labels))
}
predictLearner.classif.keraslogreg = function(.learner, .model, .newdata) {
browser()
#assignInNamespace("print.Prediction", NULL, "mlr")
if (.learner$predict.type == "response") {
p = .model$learner.model$model %>% predict_classes(x = as.matrix(.newdata))
p = factor(.model$learner.model$target_labels[p + 1])
}
if (.learner$predict.type == "prob") {
p = .model$learner.model$model %>% predict_proba(x = as.matrix(.newdata))
if (ncol(p) == 1L) p = cbind(p, 1-p)
colnames(p) = .model$learner.model$target_labels
}
return(p)
}
lrn = makeLearner("classif.keraslogreg", predict.type = "prob", epochs = 100L)
lrn = cpoDummyEncode() %>>% lrn
mod = train(lrn, bc.task)
target
target[1:10]
y[1:10]
as.integer(target) - 1
to_categorical(as.integer(target) - 1)
y[1:10,]
target[1:10]
y[1:10,]
bc.task$task.desc$positive
predict(mod, newdata =  getTaskData(bc.task))
p = .model$learner.model$model %>% predict_classes(x = as.matrix(.newdata))
p
p + 1
.model$learner.model$target_labels
str(p)
ifelse(p == 1, .model$learner.model$target_labels[1],
.model$learner.model$target_labels[2])
p[1:10]
p2 = ifelse(p == 1, .model$learner.model$target_labels[1],
.model$learner.model$target_labels[2])
p2[1:10]
factor(ifelse(p == 1, .model$learner.model$target_labels[1],
.model$learner.model$target_labels[2]), levels = .model$learner.model$target_labels)
p = .model$learner.model$model %>% predict_proba(x = as.matrix(.newdata))
p2
p2[1:10]
p[1:10]
cbind(p, 1-p)
p = cbind(p, 1-p)
p2[1:10]
p[1:10]
p[1:10,]
.model$learner.model$target_labels
colnames(p) = .model$learner.model$target_labels
library(mlr)
library(mlrCPO)
library(keras)
library(purrr)
makeRLearner.classif.keraslogreg = function() {
makeRLearnerClassif(
cl = "classif.keraslogreg",
package = "keras",
par.set = makeParamSet(
makeIntegerLearnerParam(id = "epochs", default = 30L, lower = 1L)),
par.vals = list(num.threads = 1L, verbose = FALSE, respect.unordered.factors = "order"),
properties = c("twoclass", "numerics", "factors", "prob",
"ordered"),
name = "Keras Logistic Regression",
short.name = "keraslogreg",
note = "Learner param 'predict.method' maps to 'method' in predict.keraslogreg."
)
}
trans_target = function(target) {
browser()
y = to_categorical(as.integer(target) - 1)
y = y[, 1, drop = FALSE]
return(y)
}
trainLearner.classif.keraslogreg = function(.learner, .task, .subset, .weights = NULL, ...) {
dt = getTaskData(.task)
x = as.matrix(dt[, getTaskFeatureNames(.task)])
target = dt[, getTaskTargetNames(.task)]
y = trans_target(target)
input_shape = ncol(data)
target_labels = getTaskClassLevels(.task)
output_shape = length(target_labels) - 1
model = keras_model_sequential()
model = model %>%
layer_dense(
units = output_shape,
input_shape = input_shape,
activation = "sigmoid")
compile(model, optimizer = "adam", loss = "binary_crossentropy",
metrics = "accuracy")
es = callback_early_stopping(monitor='val_loss', patience=5L)
history = invoke(keras::fit,
object = model,
x = x,
y = y,
epochs = .learner$par.vals$epochs,
batch_size = 128,
validation_split =1/3,
callbacks = list(es))
return(list(model = model, history = history, target_labels = target_labels))
}
predictLearner.classif.keraslogreg = function(.learner, .model, .newdata) {
#assignInNamespace("print.Prediction", NULL, "mlr")
if (.learner$predict.type == "response") {
p = .model$learner.model$model %>% predict_classes(x = as.matrix(.newdata))
p = factor(ifelse(p == 1, .model$learner.model$target_labels[1],
.model$learner.model$target_labels[2]), levels = .model$learner.model$target_labels)
}
if (.learner$predict.type == "prob") {
p = .model$learner.model$model %>% predict_proba(x = as.matrix(.newdata))
if (ncol(p) == 1L) p = cbind(p, 1-p)
colnames(p) = .model$learner.model$target_labels
}
return(p)
}
lrn = makeLearner("classif.keraslogreg", predict.type = "prob", epochs = 100L)
lrn = cpoDummyEncode() %>>% lrn
mod = train(lrn, bc.task)
predict(mod, newdata =  getTaskData(bc.task))
predict(mod, newdata =  getTaskData(bc.task))
predict(mod, newdata =  getTaskData(bc.task)[,getTaskFeatureNames(bc.task)])
predict(mod, newdata =  getTaskData(bc.task))
library(mlr)
library(mlrCPO)
library(keras)
library(purrr)
makeRLearner.classif.keraslogreg = function() {
makeRLearnerClassif(
cl = "classif.keraslogreg",
package = "keras",
par.set = makeParamSet(
makeIntegerLearnerParam(id = "epochs", default = 30L, lower = 1L)),
par.vals = list(epochs = 30L),
properties = c("twoclass", "numerics", "factors", "prob",
"ordered"),
name = "Keras Logistic Regression",
short.name = "keraslogreg",
note = "Learner param 'predict.method' maps to 'method' in predict.keraslogreg."
)
}
trans_target = function(target) {
y = to_categorical(as.integer(target) - 1)
y = y[, 1, drop = FALSE]
return(y)
}
trainLearner.classif.keraslogreg = function(.learner, .task, .subset, .weights = NULL, ...) {
dt = getTaskData(.task)
x = as.matrix(dt[, getTaskFeatureNames(.task)])
target = dt[, getTaskTargetNames(.task)]
y = trans_target(target)
input_shape = ncol(data)
target_labels = getTaskClassLevels(.task)
output_shape = length(target_labels) - 1
model = keras_model_sequential()
model = model %>%
layer_dense(
units = output_shape,
input_shape = input_shape,
activation = "sigmoid")
compile(model, optimizer = "adam", loss = "binary_crossentropy",
metrics = "accuracy")
es = callback_early_stopping(monitor='val_loss', patience=5L)
history = invoke(keras::fit,
object = model,
x = x,
y = y,
epochs = .learner$par.vals$epochs,
batch_size = 128,
validation_split =1/3,
callbacks = list(es))
return(list(model = model, history = history, target_labels = target_labels))
}
predictLearner.classif.keraslogreg = function(.learner, .model, .newdata) {
#assignInNamespace("print.Prediction", NULL, "mlr")
if (.learner$predict.type == "response") {
p = .model$learner.model$model %>% predict_classes(x = as.matrix(.newdata))
p = factor(ifelse(p == 1, .model$learner.model$target_labels[1],
.model$learner.model$target_labels[2]), levels = .model$learner.model$target_labels)
}
if (.learner$predict.type == "prob") {
p = .model$learner.model$model %>% predict_proba(x = as.matrix(.newdata))
if (ncol(p) == 1L) p = cbind(p, 1-p)
colnames(p) = .model$learner.model$target_labels
}
return(p)
}
lrn = makeLearner("classif.keraslogreg", predict.type = "prob", epochs = 100L)
lrn = cpoDummyEncode() %>>% lrn
mod = train(lrn, bc.task)
lrn = makeLearner("classif.keraslogreg", predict.type = "prob")
lrn = cpoDummyEncode() %>>% lrn
mod = train(lrn, bc.task)
library(mlr)
library(mlrCPO)
lrn = makeLearner("classif.ranger")
lrn = cpoScale() %>>% cpoDummyEncode() %>>% lrn
mod = train(lrn, bc.task)
predict(mod, newdata =  getTaskData(bc.task))
#### ISSUE 1:
library(mlr3)
## XGBOOST
library(mlr3learners)
task_list = list(mlr_tasks$get("iris"), mlr_tasks$get("wine"))
lrn = lrn("classif.xgboost")
## XGBOOST
library(mlr3pipelines)
library(mlr3)
library(mlr3learners)
task_list = list(mlr_tasks$get("iris"), mlr_tasks$get("wine"))
mod = po("scale") %>>%
po("encode") %>>%
po(lrn("classif.xgboost", predict_type = "prob", nthread = 1L))
#mod$keep_results = TRUE
mod = GraphLearner$new(mod)
tune_ps = paradox::ParamSet$new(list(
paradox::ParamDbl$new("classif.xgboost.nrounds", lower = 1,
upper = log(1000))))
tune_ps$trafo = function(x, param_set) {
x$classif.xgboost.nrounds = round(exp(x$classif.xgboost.nrounds))
return(x)
}
resampling = rsmp("cv", folds = 3)
measure = msr("classif.ce")
tuner = tnr("grid_search", resolution = n_evals)
terminator = term("evals", n_evals = n_evals)
library(mlr3tuning)
resampling = rsmp("cv", folds = 3)
measure = msr("classif.ce")
tuner = tnr("grid_search", resolution = n_evals)
tuner = tnr("grid_search", resolution = 3L)
terminator = term("evals", n_evals = 3L)
AutoTuner$new(mod, resampling, measure, tune_ps, terminator, tuner)
at = AutoTuner$new(mod, resampling, measure, tune_ps, terminator, tuner)
mlr_tasks$get("iris")
at$train(task = mlr_tasks$get("iris"))
plan(multicore)
at$train(task = mlr_tasks$get("iris"))
library(future)
plan(multicore)
at$train(task = mlr_tasks$get("iris"))
plan(multicore)
design = benchmark_grid(
tasks = task_list,
learners = list(at),
resamplings = rsmp("holdout", ratio = 2/3)
)
benchmark(design, store_models = TRUE)
design = benchmark_grid(
tasks = task_list,
learners = at,
resamplings = rsmp("holdout", ratio = 2/3)
)
benchmark(design, store_models = TRUE)
learner = list(
GraphLearner$new({
mod = GraphLearner$new(mod)
tune_ps = paradox::ParamSet$new(list(
paradox::ParamDbl$new("classif.xgboost.nrounds", lower = 1,
upper = log(1000))))
tune_ps$trafo = function(x, param_set) {
x$classif.xgboost.nrounds = round(exp(x$classif.xgboost.nrounds))
return(x)
}
resampling = rsmp("cv", folds = 3)
measure = msr("classif.ce")
tuner = tnr("grid_search", resolution = 3L)
terminator = term("evals", n_evals = 3L)
AutoTuner$new(mod, resampling, measure, tune_ps, terminator, tuner)
}))
plan(multicore)
design = benchmark_grid(
tasks = task_list,
learners = list(at),
resamplings = rsmp("holdout", ratio = 2/3)
)
design = benchmark_grid(
tasks = task_list,
learners =learner,
resamplings = rsmp("holdout", ratio = 2/3)
)
benchmark(design, store_models = TRUE)
task_list = list(mlr_tasks$get("iris"), mlr_tasks$get("wine"))
learner = list(
GraphLearner$new({
mod = po("scale") %>>%
po("encode") %>>%
po(lrn("classif.xgboost", predict_type = "prob", nthread = 1L))
mod = GraphLearner$new(mod)
tune_ps = paradox::ParamSet$new(list(
paradox::ParamDbl$new("classif.xgboost.nrounds", lower = 1,
upper = log(1000))))
tune_ps$trafo = function(x, param_set) {
x$classif.xgboost.nrounds = round(exp(x$classif.xgboost.nrounds))
return(x)
}
resampling = rsmp("cv", folds = 3)
measure = msr("classif.ce")
tuner = tnr("grid_search", resolution = 3L)
terminator = term("evals", n_evals = 3L)
AutoTuner$new(mod, resampling, measure, tune_ps, terminator, tuner)
}))
plan(multicore)
design = benchmark_grid(
tasks = task_list,
learners =learner,
resamplings = rsmp("holdout", ratio = 2/3)
)
benchmark(design, store_models = TRUE)
bmr = benchmark(design, store_models = TRUE)
bmr$learners
learners = list(
GraphLearner$new(
{
mod = po("scale") %>>%
po("encode") %>>%
po(lrn("classif.ranger", predict_type = "prob"))
mod$keep_results = TRUE
tune_ps =  paradox::ParamSet$new(list(
paradox::ParamDbl$new("classif.ranger.num.trees", lower = 1, upper = log(1000))))
tune_ps$trafo = function(x, param_set) {
x$classif.ranger.num.trees = round(exp(x$classif.ranger.num.trees))
return(x)
}
mod = GraphLearner$new(mod)
AutoTuner$new(mod, resampling, measure, tune_ps, terminator, tuner)
}),
GraphLearner$new(
{
mod = po("scale") %>>%
po("encode") %>>%
po(lrn("classif.xgboost", predict_type = "prob", nthread = 1L))
mod$keep_results = TRUE
tune_ps = paradox::ParamSet$new(list(
paradox::ParamDbl$new("classif.xgboost.nrounds", lower = 1,
upper = log(1000))))
tune_ps$trafo = function(x, param_set) {
x$classif.xgboost.nrounds = round(exp(x$classif.xgboost.nrounds))
return(x)
}
mod = GraphLearner$new(mod)
AutoTuner$new(mod, resampling, measure, tune_ps, terminator, tuner)
}
)
)
plan(multicore)
plan(multicore)
design = benchmark_grid(
tasks = task_list,
learners =learners,
resamplings = rsmp("holdout", ratio = 2/3)
)
bmr = benchmark(design, store_models = TRUE)
?plan
plan(multicore)
design = benchmark_grid(
tasks = task_list,
learners =learners,
resamplings = rsmp("holdout", ratio = 2/3)
)
bmr = benchmark(design, store_models = TRUE)
parallelSocket?
parallelMap::parallelStartSocket
?parallelMap::parallelStartSocket
?parallelMap::parallelMap
?parallelMap::parallelMap
?parallelMap::parallelSource
getwd()
setwd("/home/susanne/compstat/paper_2019_counterfactual_explanations/code/irace/")
getwd()
models_irace = readRDS("../saved_objects/models_irace.rds")
models_irace[[7]]$learner.id
models_irace[[12]]$learner.id
models_irace[[13]]$learner.id
models_irace[[18]]$learner.id
models_irace[[19]]$learner.id
models_irace[[22]]$learner.id
readRDS("../saved_objects/max_evals.rds")
3850/50
4200/50
readRDS("../saved_objects/max_evals.rds")
max(readRDS("../saved_objects/max_evals.rds")/50)
max = max(readRDS("../saved_objects/max_evals.rds"))
max
10000/50
500*50
max = max(readRDS("../saved_objects/max_evals_old.rds"))
max
res = readRDS("../saved_objects/max_evals.rds")
res
res/50
saveRDS(res/50, "max_evals.rds")
res = readRDS("../saved_objects/max_evals.rds")
res
res = readRDS("max_evals.rds")
res
res = readRDS("../saved_objects/max_evals.rds")
res
res = readRDS("max_evals.rds")
res = readRDS("../saved_objects/max_evals.rds")
res = readRDS("../saved_objects/max_evals.rds")
getwd()
res_old = res
res_old
res = readRDS("../saved_objects/max_eval.rds")
res
res = readRDS("../saved_objects/max_gen.rds")
res
getwd()
irace_results = readRDS("../saved_objects/irace_results.rds")
irace_results
#--- Extract best parameterset ----
# Get best parameters from irace, only keep best and round to 2 digits
best_params = irace::removeConfigurationsMetaData(irace_results)[1,]
best_params
num_id = sapply(best_params, is.numeric)
best_params[num_id] = round(best_params[num_id], 2)
best_params
readRDS("../saved_objects/best_config.rds")
readRDS("../saved_objects/best_configs.rds")
readRDS("../saved_objects/best_configs.rds")
models.5[c(11,12)]
models.5 = rep(models_irace, 5)
models_irace = readRDS("../saved_objects/models_irace.rds")
models.5 = rep(models_irace, 5)
models.5[c(11,12)]
readRDS("../saved_objects/best_configs.rds")
readRDS("../saved_objects/max_generations.rds")
models.5
readRDS("../saved_objects/max_generations.rds")
readRDS("../saved_objects/best_configs.rds")
models.5[c(31,32)]
models.5[c(21,22)]
models.5[c(19,26)]
readRDS("../saved_objects/best_configs.rds")
readRDS("../saved_objects/max_generations.rds")
readRDS("../saved_objects/max_generations.rds")
readRDS("../saved_objects/best_configs.rds")
max.gen = readRDS("../saved_objects/max_generations.rds")
max.gen
mean(max.gen)
quantile(max.gen, 0.75)
summary(max.gen)
readRDS("../saved_objects/best_configs.rds")
quantile(max.gen, 0.9)
?quantile
allgen = max.gen
quantile(allgen, 0.9, na.rm = TRUE)
best_params
generations = quantile(allgen, 0.9, na.rm = TRUE)
generations = as.integer(quantile(allgen, 0.9, na.rm = TRUE))
generations
cbind(generations, best_params)
# Save best parameter set
best_config = cbind(generations, best_params)
best_config
generations = as.integer(quantile(allgen, 0.8, na.rm = TRUE))
generations
# Save best parameter set
best_config = cbind(generations, best_params)
generations = as.integer(quantile(allgen, 0.85, na.rm = TRUE))
# Save best parameter set
best_config = cbind(generations, best_params)
best_config
generations = as.integer(quantile(allgen, 0.8, na.rm = TRUE))
# Save best parameter set
best_config = cbind(generations, best_params)
best_config
saveRDS(best_config, "../saved_objects/best_configs.rds")
readRDS("../saved_objects/best_configs.rds")
