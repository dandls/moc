 

@misc{pramit_choudhary_2018_1198885,
  author       = {Pramit Choudhary and
                  Aaron Kramer and
                  datascience.com team, contributors},
  title        = {{Skater: Model Interpretation Library}},
  month        = mar,
  year         = 2018,
  doi          = {10.5281/zenodo.1198885},
  url          = {https://doi.org/10.5281/zenodo.1198885}
}

@Manual{dalex,
    title = {DALEX: Descriptive mAchine Learning EXplanations},
    author = {Przemyslaw Biecek},
    year = {2018},
    note = {R package version 0.2.0},
    url = {https://CRAN.R-project.org/package=DALEX},
  }

  @Manual{lime,
    title = {lime: Local Interpretable Model-Agnostic Explanations},
    author = {Thomas Lin Pedersen and MichaÃ«l Benesty},
    year = {2017},
    note = {R package version 0.3.0},
    url = {https://CRAN.R-project.org/package=lime},
  }  
  
  @Article{pdp1,
    title = {pdp: An R Package for Constructing Partial Dependence
      Plots},
    author = {Brandon M. Greenwell},
    journal = {The R Journal},
    year = {2017},
    volume = {9},
    number = {1},
    pages = {421--436},
    url =
      {https://journal.r-project.org/archive/2017/RJ-2017-016/index.html},
  }

  @Article{ice,
    title = {Peeking Inside the Black Box: Visualizing Statistical
      Learning With Plots of Individual Conditional Expectation},
    author = {Alex Goldstein and Adam Kapelner and Justin Bleich and
      Emil Pitkin},
    journal = {Journal of Computational and Graphical Statistics},
    volume = {24},
    number = {1},
    pages = {44--65},
    doi = {10.1080/10618600.2014.907095},
    year = {2015},
  }

@article{friedman2008predictive,
  title={Predictive learning via rule ensembles},
  author={Friedman, Jerome H and Popescu, Bogdan E and others},
  journal={The Annals of Applied Statistics},
  volume={2},
  number={3},
  pages={916--954},
  year={2008},
  publisher={Institute of Mathematical Statistics},
  doi = {10.1214/07-AOAS148}
}



@article{strumbelj2014,
abstract = {We present a sensitivity analysis-based method for explaining prediction models that can be applied to any type of classification or regression model. Its advantage over existing general methods is that all subsets of input features are perturbed, so interactions and redundancies between features are taken into account. Furthermore, when explaining an additive model, the method is equivalent to commonly used additive model-specific methods. We illustrate the method's usefulness with examples from artificial and real-world data sets and an empirical analysis of running times. Results from a controlled experiment with 122 participants suggest that the method's explanations improved the participants' understanding of the model.},
author = {Strumbelj, Erik and Kononenko, Igor and {\v{S}}trumbelj, Erik and Kononenko, Igor},
doi = {10.1007/s10115-013-0679-x},
isbn = {0219-1377},
issn = {02193116},
journal = {Knowledge and Information Systems},
keywords = {Data mining,Decision support,Interpretability,Knowledge discovery,Visualization},
number = {3},
pages = {647--665},
title = {{Explaining prediction models and individual predictions with feature contributions}},
volume = {41},
year = {2014}
}

@inproceedings{ribeiro2016should,
  title={Why should i trust you?: Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={1135--1144},
  year={2016},
  organization={ACM},
  doi = {10.1145/2939672.2939778}
}


@article{Fisher2018,
abstract = {There are serious drawbacks to many current variable importance (VI) methods, in that they tend to not be comparable across model types, can obscure implicit assumptions about the data generating distribution, or can give seemingly incoherent results when multiple prediction models fit the data well. In this paper we propose a framework of VI measures for describing how much any model class (e.g. all linear models of dimension p), any model-fitting algorithm (e.g. Ridge regression with fixed regularization parameter), or any individual prediction model (e.g. a single linear model with fixed coefficient vector), relies on covariate(s) of interest. The building block of our approach, Model Reliance (MR), compares a prediction model's expected loss with that model's expected loss on a pair observations in which the value of the covariate of interest has been switched. Expanding on MR, we propose Model Class Reliance (MCR) as the upper and lower bounds on the degree to which any well-performing prediction model within a class may rely on a variable of interest, or set of variables of interest. Thus, MCR describes reliance on a variable while accounting for the fact that many prediction models, possibly of different parametric forms, may fit the data well. We give probabilistic bounds for MR and MCR, leveraging existing results for U-statistics. We also illustrate connections between MR, conditional causal effects, and linear regression coefficients. We outline implementations of our approaches for regularized linear regression, and to regression in a reproducing kernel Hilbert space. We then apply MR {\&} MCR to study the behavior of recidivism prediction models, using a public dataset of Broward County criminal records.},
archivePrefix = {arXiv},
arxivId = {1801.01489},
author = {Fisher, Aaron and Rudin, Cynthia and Dominici, Francesca},
eprint = {1801.01489},
title = {{Model Class Reliance: Variable Importance Measures for any Machine Learning Model Class, from the "Rashomon" Perspective}},
url = {http://arxiv.org/abs/1801.01489},
year = {2018}
}


@Manual{R,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2016},
    url = {https://www.R-project.org/},
}

@article{friedman2001greedy,
  title={Greedy function approximation: a gradient boosting machine},
  author={Friedman, Jerome H},
  journal={Annals of statistics},
  pages={1189--1232},
  year={2001},
  publisher={JSTOR},
  doi = {10.1214/aos/1013203451 }
}
